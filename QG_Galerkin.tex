\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{nicefrac}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\bu}{\bm{u}}
\newcommand{\eps}{\epsilon}
\newcommand{\pd}[1]{\partial_{#1}}
\newcommand{\dd}[3]{\frac{\text{d}^{#3}#1}{\text{d}#2^{#3}}}
\newcommand{\tJ}{\text{J}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\mat}[1]{\text{\bf #1}}
\newcommand{\imag}{\text{i}}
\title{Finite-dimensional approximations to QG dynamics}
\author{Grooms, Julien, Watwood}
\date{Last Update: \today}
\begin{document}
\maketitle

The traditional inviscid, unforced continuously-stratified QG equations are
\begin{gather}
\pd{t}\vartheta^+ + \tJ[\psi^+,\vartheta^+] = 0\\
\pd{t}q+\tJ[\psi,q]+\beta v = 0\\
\pd{t}\vartheta^-+ \tJ[\psi^-,\vartheta^-] = 0\\
\nabla_h^2\psi + \pd{z}\left(S(z)\pd{z}\psi\right) = q,\quad S(z) = \frac{f_0^2}{H^2N^2(z)}
\end{gather}
where 
\begin{equation}
\vartheta^\pm= S(z)\pd{z}\psi\text{ at }z=0,1
\end{equation}
Notation follows Rocha et al.~(JPO 2016; RYG16).
I have assumed that the dimensional $z$ goes from $0$ to $H$, and the above equations use a nondimensional $z$ that goes from 0 to 1.\\

\hrule
\begin{center}{\bf Energy conservation and Galerkin approximation}\end{center}

We want an approximate solution that has the following form
\begin{equation}
q_\sN^G = \sum_{n=1}^\sN \breve{q}_n(x,y,t)p_n^q(z),\;\;\psi_\sN^G = \sum_{i=1}^\sN \breve\psi_i(x,y,t)p_n^\psi(z)
\end{equation}
where $p_n^q(z)$ and $p_n^\psi(z)$ are a basis functions.
E.g.~we could use the modal expansion of RYG16, or any basis for the space of polynomials of degree $\le n$, or finite elements, etc.
The basis for $q$ does not need to be the same as the basis for $\psi$.
If we insert these above we get
\begin{gather}
\pd{t}q_\sN^G+\tJ[\psi_\sN^G,q_\sN^G]+\beta \pd{x}\psi_\sN^G= r_{qt}\neq0\\
q_\sN^G-\left(\nabla_h^2\psi_\sN^G + \pd{z}\left(S(z)\pd{z}\psi_\sN^G\right)\right)=r_q\neq0\\
\vartheta^++r_\vartheta^+= S(z)\pd{z}\psi_\sN^G\text{ at }z=1,\text{ and }\vartheta^-+r_\vartheta^- = S(z)\pd{z}\psi_\sN^G\text{ at }z=0.
\end{gather}
I allow some error in the boundary conditions.
We don't need to allow for errors in the evolution of $\vartheta^\pm$ because that equation does not directly depend on the vertical structure.
To put it another way, it is straightforward to enforce the following exact evolution equations:
%We can make two kinds of errors on the boundaries:
%\begin{itemize}
%\item Require $(f_0/N)^2\pd{z}\psi_\sN^G= \vartheta^\pm$ exactly, but allow errors in the evolution of $\vartheta^\pm$, or
%\item Have no errors in the evolution of $\vartheta^\pm$ but allow errors such that $(f_0/N)^2\pd{z}\psi_\sN^G \neq \vartheta^\pm$.
%\end{itemize}
%These kinds of errors are interchangeable in the following sense: if you set $\vartheta^\pm = (f_0/N)^2\pd{z}\psi_\sN^G$ then there will be errors in the evolution of $\vartheta^\pm$, whereas if you define $\vartheta^\pm$ to have no errors in its evolution then $\vartheta^\pm \neq (f_0/N)^2\pd{z}\psi_\sN^G$.
%For the sake of convenience I will assume that $\vartheta^\pm \neq (f_0/N)^2\pd{z}\psi_\sN^G$:
\begin{gather}
\pd{t}\vartheta^+ + \tJ[\psi_\sN^G,\vartheta^+] =  0\\
\pd{t}\vartheta^-+ \tJ[\psi_\sN^G,\vartheta^-]= 0.
\end{gather}
Note that because the Galerkin approximation $\psi_\sN^G$ is not equal to the true solution $\psi$ there will be a different kind of errors in the evolution of $\vartheta^\pm$, but we can still enforce the above equations to hold exactly.

\clearpage
We want our equations to conserve energy in the form $1/2\int |\nabla\psi_\sN^G|^2+S(z)(\pd{z}\psi_\sN^G)^2$.
The standard approach to proving energy conservation is to multiply equation (7) for $\pd{t}q_\sN^G$ by $-\psi_\sN^G$, then use equation (8) to replace $q_\sN^G$, then perform multiple integrations by parts to arrive at an expression for the evolution of energy:
\[\int-\psi_\sN^G\pd{t}\left(\nabla_h^2\psi_\sN^G + \pd{z}\left(S(z)\pd{z}\psi_\sN^G\right) + r_q\right) = \int\psi_\sN^G(\tJ[\psi_\sN^G,q_\sN^G] - r_{qt}) = -\int\psi_\sN^Gr_{qt}.\]
The hope is that we can impose appropriate conditions on $r_q$ and $r_{qt}$ such that the energy is conserved.
E.g.~a standard Galerkin condition would be to require both $r_q$ and $r_{qt}$ to be orthogonal to the span of the basis functions.

We simplify the above expression piece by piece.
First, assuming appropriate lateral boundary conditions, we have
\[\int -\psi_\sN^G\pd{t}\nabla_h^2\psi_\sN^G = \frac{1}{2}\dd{}{t}{}\int|\nabla_h\psi_\sN^G|^2.\]
Next
\begin{align}
\int-\psi_\sN^G\pd{z}\left(S(z)\pd{zt}\psi_\sN^G\right) &= -\int_x\left[S(z)\psi_\sN^G\pd{zt}\psi_\sN^G\right]_-^+ + \frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2\\
&=  -\int_x\left[\psi_\sN^G(\pd{t}\vartheta+\pd{t}r_\vartheta)\right]_-^+ + \frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2\\
&=  -\int_x\left[\psi_\sN^G(-\tJ[\psi_\sN^G,\vartheta]+\pd{t}r_\vartheta)\right]_-^+ + \frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2\\
&=-\int_x\left[\psi_\sN^G\pd{t}r_\vartheta\right]_-^++ \frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2.
\end{align}
Putting it all together,
\[\frac{1}{2}\dd{}{t}{}\int |\nabla_h\psi_\sN^G|^2 + S(z)\left(\pd{z}\psi_\sN^G\right)^2 =\int_x\left[\psi_\sN^G\pd{t}r_\vartheta\right]_-^++ \int\psi_\sN^G\pd{t}r_q-\int\psi_\sN^Gr_{qt}.\]

The idea is that the coefficients $\breve q_n$ and the boundary values $\vartheta^\pm$ are known, but the coefficients $\breve \psi_n$ and $\pd{t}\breve q_n$ are unknown.
We can try to specify these coefficients to achieve the dual purposes of accuracy and energy conservation.
Suppose we wish to impose the usual Galerkin conditions:
First that $r_q$ is orthogonal to the $\sN$ basis functions $p_n^\psi$, which gives us $\sN$ constraints on the PV inversion, then that $r_{qt}$ is also orthogonal to the $\sN$ basis functions $p_n^\psi$, which gives us another $\sN$ constraints on the PV evolution.
That will leave zero degrees of freedom to impose the boundary conditions, i.e.~it won't be possible to set the error on the boundary $r_\vartheta^\pm$ to zero, and we won't be able to conserve energy.
This seems to be a severe obstacle to energy conservation, since we can set the terms corresponding to $r_q$ and $r_{qt}$ to zero in the energy conservation equation, but we appear to be unable to control the term corresponding to $r_\vartheta^\pm$.
`Approximation B' from RYG16 is the Tulloch \& Smith 2009 model; it enforces $r_\vartheta^\pm=0$ and $r_q=0$, but then doesn't have enough remaining degrees of freedom to set the terms corresponding to $r_{qt}$ to zero.\\

On the other hand, RYG16 was able to achieve an energy-conserving Galerkin formulation; how was this possible?
First, the `modal' basis functions in that paper have $\pd{z}p_n=0$ at the surfaces, which means that $r_{\vartheta}^\pm = -\vartheta^\pm$.
As a result, $[\int_x\psi_\sN^G\pd{t}r_\vartheta]_-^+ = -[\int_x\psi_\sN^G\pd{t}\vartheta]_-^+=0$. %In `approximation B' the residual $r_q$ is zero, and energy is therefore conserved, but the boundary $\vartheta$ are dynamically passive in the sense that they have no impact on the evolution of $q_\sN^G$.
In `approximation C' from RYG16 the boundary information is included in the PV inversion via delta-function sheets of PV at the boundary.
This relies on the following fact, first noted by Bretherton (1966):
\begin{quote}
The solution $\psi$ to (4)-(5) (with inhomogeneous Neumann boundary conditions) is the same as the solution to the following PV inversion
\begin{equation}
q -\vartheta^+\delta(z-1) + \vartheta^-\delta(z) = \nabla_h^2\psi + \pd{z}\left(S(z)\pd{z}\psi\right)
\end{equation}
with homogeneous Neumann boundary conditions $\pd{z}\psi=0$.
\end{quote}

We will now generalize `Approximation C' from RYG16 to a general basis.
Suppose only that our basis $p_n^\psi$ has d$p_n^\psi(z)/$d$z = 0$ at the boundaries.
Inserting our ansatz into the Bretherton PV inversion we find a new residual $r_q$
\begin{equation}
q_\sN^G -\vartheta^+\delta(z-1) + \vartheta^-\delta(z) = \nabla_h^2\psi_\sN^G + \pd{z}\left(S(z)\pd{z}\psi_\sN^G\right) + r_q^B.
\end{equation}
The superscript $B$ stands for `Bretherton' and emphasizes that this residual is different from $r_q$.
Repeating the above analysis with the new definition of the residual $r_q^B$:
\[\int -\psi_\sN^G\pd{t}q_\sN^G = \int -\psi_\sN^G\pd{t}\left(\nabla_h^2\psi_\sN^G + \pd{z}\left(S(z)\pd{z}\psi_\sN^G\right)+\vartheta^+\delta(z-1)-\vartheta^-\delta(z)+r_q^B\right).\]
Again, 
\[\int -\psi_\sN^G\pd{t}\nabla_h^2\psi_\sN^G = \frac{1}{2}\dd{}{t}{}\int|\nabla_h\psi_\sN^G|^2.\]
As before
\begin{align}
\int-\psi_\sN^G\pd{z}\left(S(z)\pd{zt}\psi_\sN^G\right) &= -\int_x\left[S(z)\psi_\sN^G\pd{zt}\psi_\sN^G\right]_0^1 + \frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2=\frac{1}{2}\dd{}{t}{}\int S(z)\left(\pd{z}\psi_\sN^G\right)^2.
\end{align}
This time we used the fact that $\pd{z}\psi_\sN^G = 0$ on the boundaries.
We now have a new term
\[\int-\psi_\sN^G(\pd{t}\vartheta^+\delta(z-1)-\pd{t}\vartheta^-\delta(z)) = -\int_x[\psi_\sN^G\pd{t}\vartheta]_0^1 = 0.\]
The zero is because $\pd{t}\vartheta^\pm = -\tJ[\psi_\sN^G,\vartheta^\pm]$; multiplying by $\psi_\sN^G$ and integrating yields 0.
This leaves
\[\int -\psi_\sN^G\pd{t}q_\sN^G= \frac{1}{2}\dd{}{t}{}\int \left[|\nabla_h\psi_\sN^G|^2 + S(z)\left(\pd{z}\psi_\sN^G\right)^2\right] - \int\psi_\sN^G\pd{t}r_q^B.\]
The energy equation is now
\[\frac{1}{2}\dd{}{t}{}\int |\nabla_h\psi_\sN^G|^2 + S(z)\left(\pd{z}\psi_\sN^G\right)^2 = \int\psi_\sN^G\pd{t}r_q^B-\int\psi_\sN^Gr_{qt}.\]
To conserve energy we simply impose the usual Galerkin conditions that the residuals $r_q^B$ and $r_{qt}$ are orthogonal to the span of the basis functions $p_n^\psi$.

To be precise, if the basis for $q$ is the same as the basis for $\psi$ then we are imposing a traditional Galerkin condition.
But there is no reason why we should enforce $\pd{z}q_\sN^G=0$ at the boundary, so it is probably advantageous to use a more general basis for $q$ than for $\psi$.
In this case we are enforcing one Galerkin condition and one Petrov-Galerkin condition.
A Galerkin condition requires a residual to be orthogonal to the space in which an approximate solution is sought.
The residual in the PV equation $r_q^B$ is required to be orthogonal to the span of $p_n^\psi$, and when solving the PV inversion we are seeking a solution $\psi_\sN^G$ in the span of the $p_n^\psi$, so this is a Galerkin condition.
The solution we obtain for $\psi_\sN^G$ by imposing the Galerkin condition is optimal in the sense that it minimizes
\[-\int(\psi-\psi_\sN^G)\left[\nabla^2(\psi-\psi_\sN^G) + \pd{z}\left(S(z)\pd{z}(\psi-\psi_\sN^G)\right)\right]\]
over all functions $\psi_\sN^G$ in the subspace.
RYG16 makes it look like we're minimizing the $L^2$ norm of the error in $\psi$, which is not the case.

A Petrov-Galerkin condition requires a residual to be orthogonal to a {\it different} subspace than the subspace in which a solution is sought.
In the PV evolution equation we are seeking a solution for $\pd{t}q_\sN^G$ that is in the span of $p_n^q$ but requiring the residual $r_{qt}$ to be orthogonal to the span of the $p_n^\psi$, so this is a Petrov-Galerkin condition.
In RYG16 the same basis is used for $\psi$ and $q$, so it's a Galerkin condition, and in that case the approximation is optimal in the $L^2$ norm.
I.e.~the approximation $\pd{t}q_\sN^G$ is chosen to be as close as possible to $-\text{J}[\psi_\sN^G,q_\sN^G]$ in the $L^2$ norm.
We could do that here if we wanted to by setting $p_n^\psi=p_n^q$.

\clearpage
\hrule
\begin{center}{\bf Implementation with a generic Galerkin basis}\end{center}
The following discussion covers how to implement this method for any Galerkin basis, i.e.~it applied to the modal basis, to polynomials, to finite elements including DG, etc.
For simplicity of exposition assume that the horizontal directions will be periodic and make use of the Fourier tranform so that
\[\hat{q}_n(\bm{k},t),\quad\hat{\psi}_n(\bm{k},t)\]
are the Fourier transforms of $\breve q_n$ and $\breve\psi_n$, respectively, and define vectors $\hat{\bm{q}}$ and $\hat{\bm{\psi}}$ whose $n^\text{th}$ elements are $\hat{q}_n(\bm{k},t)$ and $\hat{\psi}_n(\bm{k},t)$, respectively, for $n=1,\ldots,\sN$.
Using the Galerkin conditions, the PV inversion takes the form
\[\mat{B}\hat{\bm{q}} -\hat{\vartheta}^+\bm{p}^++\hat{\vartheta}^-\bm{p}^- = -k^2\mat{M}\hat{\bm{\psi}}-\mat{L}\hat{\bm{\psi}}\]
where
\[\left(\bm{p}^+\right)_j = p_j^\psi(1),\;\;\left(\bm{p}^-\right)_j = p_j^\psi(0),\]
\[\mat{B}_{ij} = \int p_i^\psi(z)p_j^q(z)\text{d}z,\;\;\mat{M}_{ij}=\int p_i^\psi(z)p_j^\psi(z)\text{d}z,\;\;\mat{L}_{ij} = \int S(z)(\pd{z}p_i^\psi(z))(\pd{z}p_j^\psi(z))\text{d}z\]
(in the right expression I assumed $\pd{z}p_i^\psi(z)=0$ at the boundaries and integrated by parts.)
This is very similar to the standard finite-difference approach (see Grooms and Nadeau, Fluids 2017) where $\mat{M}$ and $\mat{B}$ would be the identity and $\mat{L}$ would be tridiagonal.
In the modal basis (not polynomial) the basis functions are orthogonal so $\mat{M}=\mat{B}$ is diagonal, and they are eigenfunctions so $\mat{L}$ is just $\kappa_n^2\mat{I}$.
Regardless of which basis you choose, you should evaluate the elements of $\mat{B}$, $\mat{M}$ and $\mat{L}$ to machine precision, either analytically if possible, or with adaptive quadrature or Gaussian quadrature with sufficient nodes.
Notice that the matrix $\mat{L}$ is a Gram matrix based on the functions $\pd{z}p_n^\psi$.
If $p_1^\psi = 1$ is a basis function then the set of functions $\pd{z}p_n^\psi$ is linearly dependent and $\mat{L}$ will be positive semi-definite.
Similarly, $\mat{M}$ is a Gram matrix for the functions $p_n^\psi$, so it is symmetric positive definite.\\

We need a fast way to repeatedly solve the system for $\psi$.
We need to solve for lots of different values of $k^2$, as well as repeatedly in time.
To rapidly solve the PV inversion we can first compute the Cholesky factorization of $\mat{M}$ and store it: $\mat{M} = \mat{GG}^T$.
Then note
\[-k^2\mat{M} -\mat{L} = -\mat{G}(k^2\mat{I}+\mat{G}^{-1}\mat{LG}^{-T})\mat{G}^T.\]
The matrix $\mat{G}^{-1}\mat{LG}^{-T}$ is symmetric (and positive semi-definite) so it has an orthogonal eigenvector decomposition
\[\mat{G}^{-1}\mat{LG}^{-T} = \mat{QD}\mat{Q}^{T}\]
where $\mat{D}$ is diagonal with non-negative elements and $\mat{Q}$ is an orthogonal matrix.
This allows us to write
\[\mat{B}\hat{\bm{q}} + \text{ (boundary terms) } = -\mat{GQ}(k^2\mat{I}+\mat{D})\mat{Q}^{T}\mat{G}^{T}\hat{\bm{\psi}}.\]
To obtain the solution:
\begin{itemize}
\item Compute $\mat{Q}^T\mat{G}^{-1}(\mat{B}\hat{\bm{q}} + \text{ (boundary terms) })$.
\item Next multiply the previous result by $-(k^2\mat{I}+\mat{D})^{-1}$ from the left. 
\item Finally go back to the original basis: multiply the previous result by $\mat{G}^{-T}\mat{Q}$ from the left. 
\end{itemize}
This is analogous to the approach taken in finite-difference approximations of the vertical direction.

The above analysis shows that there is a diagonalizing basis.
This diagonalizing basis approximates the modal basis.
The elements of a column of the matrix $\mat{G}^{-T}\mat{Q}$ are the coordinates of a diagonalizing basis function with respect to the basis $p_n^\psi$.
As $\sN$ increases we expect these diagonalizing basis functions to converge to the baroclinic modes of the full problem.
This is essentially the same as what happens in the finite-difference approximation, where the eigenvectors of the matrix {\bf L} are the `discrete' baroclinic modes.\\

The (Petrov)-Galerkin conditions  define how the PV coefficients $\breve{q}_n$ should evolve.
Define the vector $\breve{\bm{q}}$ to have elements $\breve{q}_n$, and define the vector $\mat{NL}$ to have elements 
\[\mat{NL}_n = \int p_n^\psi(z)\tJ[\psi_\sN^G,q_\sN^G]\text{d}z.\]
The PV evolution then takes the form
\[\mat{B}\dd{}{t}{}\breve{\bm{q}} + \mat{NL} (+\beta \text{i} k_x\hat{\bm{\psi}}) = 0.\]
%where $\mat{B}_{ij} = \int_0^1p_i^\psi(z)p_j^q(z)$d$z$.
In a fully-nonlinear implementation, one would need to repeatedly evaluate the integrals defining the elements of {\bf NL}.
This could be done via quadrature.
The LU factorization of the matrix {\bf B} could be computed once, then stored.\\

\hrule
\begin{center}{\bf Polynomial bases}\end{center}

The above considerations do not rely on any particular basis.
We now specialize to polynomials.
The set of polynomials of degree $\le \sN+1$ and with $\pd{z}p=0$ at the boundaries is a vector space of dimension $\sN$, with an infinite number of bases, any of which could be used in the above analysis.
Shen (SIAM J Sci Comput 1994; section 4) gives a basis for the space of polynomials of degree $\le \sN+1$ and with $\pd{z}p_n^\psi=0$ at the boundaries using a re-combination of Legendre polynomials.
For this basis the matrix {\bf M} is pentadiagonal, with the further property that an even/odd permutation will bring it into a block-tridiagonal form.
This matrix has condition number about $6\times10^5$ for $\sN=1000$, which is quite good.
The matrix {\bf L} depends on the stratification $S(z)=f_0^2/N^2(z)$ and in general will be dense.
(If we used a finite-element basis then {\bf L} would be sparse.)
If we use the Shen basis for $p_n^\psi$ and the standard Legendre basis for $p_n^q$ then the matrix {\bf B} is upper-triangular with upper bandwidth 2, so we wouldn't even need to compute the LU factorization and solving for the time-tendency of PV would only take $\mathcal{O}(\sN)$ flops.
There's no benefit to using Chebyshev since energy conservation requires us to use the standard $L^2$ inner product, and Chebyshev polynomials are not orthogonal in the standard $L^2$ inner product.

The overall cost of the PV inversion using the Shen and Legendre bases is as follows.
Pre-computing the Cholesky factor of {\bf M} is $\mathcal{O}(\sN)$ because the matrix is banded.
Computing the eigenvalue decomposition of $\mat{G}^{-1}\mat{L}\mat{G}^{-T}$ with the basic QR algorithm should converge quickly and not cost much per iteration because the matrix is symmetric and presumably has separated eigenvalues.
\begin{itemize}
\item Move into the diagonalizing basis: Compute $\mat{Q}^T\mat{G}^{-1}(\mat{B}\hat{\bm{q}} + \text{ (boundary terms) })$. Multiplication by {\bf B} is $\mathcal{O}(\sN)$. The cost to invert the Cholesky is $\mathcal{O}(\sN)$. The cost to multiply by {\bf Q}$^T$ is $\mathcal{O}(\sN^2)$. 
\item Invert in the diagonalizing basis: Multiply the previous result by $-(k^2\mat{I}+\mat{D})^{-1}$ from the left. This converts from $q$ to $\psi$ in the diagonalizing basis. Cost is $\mathcal{O}(\sN)$.
\item Finally go back to the original basis: multiply the previous result by $\mat{G}^{-T}\mat{Q}$ from the left. Cost is $\mathcal{O}(\sN^2)$ to multiply by {\bf Q} and $\mathcal{O}(\sN)$ to invert the Cholesky.
\end{itemize}
Once the requisite decompositions have been pre-computed the inversion cost is $\mathcal{O}(\sN^2)$.
Of course, there's no particular reason why we need to go back and forth from the Shen basis to the diagonalizing basis.
We could just start with the Shen basis, then compute the diagonalizing basis, then stick with the diagonalizing basis from then on.
If so, the cost to invert is just $\mathcal{O}(\sN)$.\\

Note that the integral for {\bf NL} is a product of three polynomials.
If the basis $p_n^q$ goes to degree $\sN-1$ and the basis $p_n^\psi$ goes to degree $\sN+1$ then the product can have degree up to $3\sN+1$.
These integrals can be evaluated exactly using Gauss-Legendre quadrature with $1.5\sN+1$ quadrature nodes.
But we need to evaluate $\psi$ at the boundaries so that we can evolve $\vartheta^\pm$ on the boundaries.
We could use Gauss-Legendre-Lobatto quadrature instead; overall it would require $1.5\sN+2$ quadrature nodes.
If we used Gauss-Legendre then we would need to evaluate $\psi$ at $1.5\sN+1$ interior points {\it and} at the boundaries, and we would need to evaluate $q$ at $1.5\sN+1$ interior points for a total of $3\sN+4$ polynomial evaluations.
If we used Gauss-Legendre-Lobatto then we would need to evaluate $\psi$ and $q$ at $1.5\sN+2$ points for a total of $3\sN+4$ polynomial evaluations.
So ultimately it's the same number of polynomial evaluations.
Gauss-Legendre is easier than Lobatto.

The one drawback to using something other than Chebyshev polynomials is that it costs $\mathcal{O}(\sN^2)$ flops to evaluate the polynomial at $\sN$ points (vs $\sN$ log$(\sN)$ for Chebyshev).
We should, as mentioned above, just use the diagonalizing basis instead of the Shen basis.
If we do that then we need to compute the matrix corresponding to the map from coordinates of a polynomial in the diagonalizing basis to values of the polynomial at the quadrature nodes.
Then, to move from the coordinates in the diagonalizing basis to the values at the quadrature nodes will cost $\mathcal{O}(\sN^2)$ and can be achieved via matrix/vector multiplication.

Overall the algorithm would be 
\begin{itemize}
\item Start from the Shen (1994) and Legendre bases, then compute the diagonalizing basis. (The columns of the matrix $\mat{G}^{-T}\mat{Q}$ are the coefficients [in the Shen (1994) basis] of the diagonalizing basis that approximates the baroclinic modes.)
\item Construct the matrix that maps from (coordinates in the diagonalizing basis) $\to $ (values on the Gauss-Legendre quadrature nodes)
\item Evolve the system in time by updating $q$ and $\vartheta^\pm$, then solving for $\psi$, etc.
\end{itemize}

In addition to the linear instability problem we should look at how rapidly the modes and deformation radii converge in the Galerkin vs FD methods and at the convergence of the interaction coefficients.\\

\hrule
\begin{center}{\bf Linear stability problem}\end{center}
The following is an exact solution of the fully-nonlinear QG equations: 
\[\bar\psi = -\bar u(z)y,\;\;\bar q = -y\dd{}{z}{}\left(S(z)\dd{\bar u}{z}{}\right),\;\;\bar\vartheta^\pm = -yS(z)\dd{\bar u^\pm}{z}{}.\]
We can linearize the PDE about this equilibrium solution to see whether it is stable/unstable to small perturbations.
Instability of this kind of equilibrium in the QG equations is one example of something called `baroclinic' instability.
The linearized equations are
\begin{gather}
\pd{t}\vartheta^+ + \bar u^+\pd{x}\vartheta^++(\pd{x}\psi^+)\pd{y}\bar\vartheta^+ = 0\\
\pd{t}q+\bar u(z) \pd{x}q + (\pd{x}\psi)\pd{y}\bar q+\beta (\pd{x}\psi) = 0\\
\pd{t}\vartheta^- + \bar u^-\pd{x}\vartheta^-+(\pd{x}\psi^-)\pd{y}\bar\vartheta^- = 0\\
\nabla^2\psi + \dd{}{z}{}\left(S(z)\dd{\psi}{z}{}\right) = q-\vartheta^+\delta(z-1)+\vartheta^-\delta(z)
\end{gather}
Coefficients don't vary in the horizontal, so we take the Fourier transform
\begin{gather}
\pd{t}\hat{\vartheta}^+ + \imag k_x\bar u^+\hat{\vartheta}^++\imag k_x(\pd{y}\bar\vartheta^+)\hat{\psi}^+ = 0\\
\pd{t}\hat{q}+\imag k_x\bar u(z) \hat{q} + \imag k_x(\pd{y}\bar{q})\hat{\psi}+\imag k_x\beta \hat{\psi} = 0\\
\pd{t}\hat{\vartheta}^- + \imag k_x\bar u^-\hat{\vartheta}^-+\imag k_x(\pd{y}\bar\vartheta^-)\hat{\psi}^- = 0\\
-(k_x^2+k_y^2)\hat{\psi} + \dd{}{z}{}\left(S(z)\dd{\hat{\psi}}{z}{}\right) = \hat{q} -\hat{\vartheta}^+\delta(z-1)+\hat{\vartheta}^-\delta(z).
\end{gather}
We look for exponential growth so we try to find solutions with $\pd{t}\hat{q} = -\imag k_x c\hat{q}$
\begin{gather}
-c\hat{\vartheta}^+ + \bar u^+\hat{\vartheta}^++(\pd{y}\bar\vartheta^+)\hat{\psi}^+ = 0\\
-c\hat{q}+\bar u(z) \hat{q} +(\pd{y}\bar q)\hat{\psi}+\beta \hat{\psi} = 0\\
-c\hat{\vartheta}^- + \bar u^-\hat{\vartheta}^-+(\pd{y}\bar\vartheta^-)\hat{\psi}^- = 0\\
-(k_x^2+k_y^2)\hat{\psi} + \dd{}{z}{}\left(S(z)\dd{\hat{\psi}}{z}{}\right) = \hat{q} -\hat{\vartheta}^+\delta(z-1)+\hat{\vartheta}^-\delta(z).
\end{gather}
For some careful choices of $\bar u(z)$ and $N^2(z)$ the equations can be solved analytically.
For example if $\bar u = z$, $\beta=0$, and $N^2(z) = N^2$ it is the `Eady' problem.
More generally you have to discretize and then solve an eigenvalue problem to find $c$.\\


The previous derivation of the linear stability problem did not adhere correctly to the philosophy of `Approximation C' from RYG16.
The previous derivation considered the background velocity $\bar u$ to be the fundamental quantity from which others are derived.
The natural viewpoint, taken by Approximation C, is that $q$ and $\vartheta^\pm$ are fundamental and that $\psi$ is uniquely derived from them.
The following should be a correct formulation.

To use our Galerkin formulation we need to represent the equilibrium solution using our Galerkin bases.
The first step would be to compute the Galerkin coefficients of $\pd{y}\bar q$, which are 
\begin{equation}
(\pd{y}\bar q)_n = \frac{\int_0^1 p_n^q(z)(\pd{y}\bar q)\text{d}z}{\int_0^1 (p_n^q(z))^2\text{d}z}.
\end{equation}
(Note that the above expression assumes that the basis functions $p_n^q$ are $L^2$-orthogonal.)
The Eady equilibrium has $\bar q=0$, which can be represented exactly in our Galerkin basis (and any basis).
Our standard PV inversion says that we can obtain the Galerkin coefficients of $\bar u$ (in the basis $p_n^\psi$) by solving the system
\begin{equation}
\mat{L}\bar{\bm{u}} =\mat{B}(\pd{y}\bar{\bm{q}}) -(\pd{y}\bar{\vartheta}^+)\bm{p}^++(\pd{y}\bar{\vartheta}^-)\bm{p}^-.
\end{equation}
As noted previously, the {\bf L} matrix is singular.
This is natural because {\bf L} is the discrete version of the operator $\pd{z}(S(z)\pd{z}\cdot)$ with homogeneous Neumann boundary conditions, which has constant functions in its null space.
The null space of {\bf L} is also one-dimensional and corresponds to constant functions (for a polynomial basis).

The Fredholm alternative tells us that a solution will exist whenever the right hand side is orthogonal to the cokernel, and since {\bf L} is symmetric the cokernel is the same as the kernel, which includes constant functions.
More simply, the first row (and column) of {\bf L} is zero, so we need to ensure that the first entry of the RHS is also zero.
The first row of the right hand side is the integral of $\phi_0(z) = 1$ times $\bar q_\sN^G - \bar\vartheta^+\delta(z-1)+\bar\vartheta^-\delta(z)$.
It turns out that this is zero by construction (I will not make the argument here), though the quadrature might have something nonzero due to quadrature and/or roundoff errors.
This shows that the RHS is always in the range of {\bf L}, so a solution always exists.

At this point we know that a solution to (32) always exists, but there are in fact an infinite number of solutions.
To arrive at a unique solution we need to constrain the null space, i.e.~we need to constrain the depth-independent (aka barotropic) part of $\bar u$.
One natural choice would be to simply compute the first Galerkin coefficient of $\bar u$ (i.e. the coefficient corresponding to the basis function $p_1^\psi(z) = \phi_0(z) = 1$) using the integral definition.
So the first element of the vector $\bar{\bm{u}}$ is 
\[\int_0^1p_1^\psi(z)\bar{u}(z)\text{d}z.\]
The remaining elements can be found by solving (32) ignoring the first row and column.

We next discretize (27)--(29).
The streamfunction $\hat{\psi}$ appears in these equations, but we only want evolution equations involving our basic/fundamental variables $\hat{q}$ and $\hat{\vartheta}^\pm$, so we formally eliminate $\hat{\psi}$ using
\[\hat{\bm{\psi}} = -\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B}\hat{\bm{q}} +\hat{\vartheta}^+\bm{\psi}^+-\hat{\vartheta}^-\bm{\psi}^-.\]
The vectors $\bm{\psi}^\pm$ are
\[\bm{\psi}^+ = \left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\bm{p}^+,\;\;\bm{\psi}^- = \left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\bm{p}^-.\]
We can now discretize the surface equations (27) and (29).
These don't require Galerkin projection, and have the form
\begin{gather}\notag
(-c+\bar u_\sN^G(z=1))\hat{\vartheta}^+ + (\pd{y}\bar\vartheta^+)\bm{p}^+\cdot\hat{\bm{\psi}}=0\\
(-c+\bar u_\sN^G(z=1))\hat{\vartheta}^+ + (\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot(-\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B}\hat{\bm{q}} +\hat{\vartheta}^+\bm{\psi}^+-\hat{\vartheta}^-\bm{\psi}^-))=0\\\notag
(-c+\bar u_\sN^G(z=0))\hat{\vartheta}^- + (\pd{y}\bar\vartheta^-)\bm{p}^-\cdot\hat{\bm{\psi}}=0\\
(-c+\bar u_\sN^G(z=0))\hat{\vartheta}^- + (\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot(-\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B}\hat{\bm{q}} +\hat{\vartheta}^+\bm{\psi}^+-\hat{\vartheta}^-\bm{\psi}^-))=0
\end{gather}

Next we discretize the PV evolution equation (28) by inserting our Galerkin approximation and integrating against the basis functions $p_n^\psi$
\begin{gather}
-c\mat{B}\hat{\bm{q}} +\left[\bar{\mat{U}} -(\bar{\mat{Q}}_y+\beta\mat{M})\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1} \mat{B}\right]\hat{\bm{q}} + (\bar{\mat{Q}}_y+\beta\mat{M})(\hat{\vartheta}^+\bm{\psi}^+-\hat{\vartheta}^-\bm{\psi}^-) = 0
\end{gather}
\[\left(\bar{\mat{U}}\right)_{jk} = \int_0^1 p_j^\psi(z)p_k^q(z)\bar{u}_\sN^G(z)\text{d}z,\;\;\left(\bar{\mat{Q}}_y\right)_{jk} = \int_0^1(\pd{y}\bar q_\sN^G)p_j^\psi(z)p_k^\psi(z)\text{d}z\]

We can now write a generalized eigenvalue problem for $c$ as follows
\begin{multline}
\left[\begin{array}{c|c|c}
\bar{u}_\sN^G(z=1)+(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^+)&-(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B})&-(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^-)\\\hline
(\bar{\mat{Q}}_y+\beta\mat{M})\bm{\psi}^+&\bar{\mat{U}} -(\bar{\mat{Q}}_y+\beta\mat{M})\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1} \mat{B}&-(\bar{\mat{Q}}_y+\beta\mat{M})\bm{\psi}^-\\\hline
 (\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^+)&-(\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B})&\bar{u}_\sN^G(z=0)-(\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^-)
\end{array}\right]\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{q}\\\hat{\vartheta}^-\end{array}\right) = \\
c\left[\begin{array}{c|c|c}
1&0&0\\\hline
0&\mat{B}&0\\\hline
0&0&1
\end{array}\right]\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{q}\\\hat{\vartheta}^-\end{array}\right)
\end{multline}

The linear instability analysis will in general proceed as follows
\begin{itemize}
\item Choose $\bar{u}$ and $N^2(z)$. (and $f_0$ and $H$ and $\beta$ and $\sN$) Use this to construct $\bar{q}$ and $\bar{\vartheta}^\pm$. Do this step analytically.
\item Use (31) and (32) to construct the Galerkin approximations $\pd{y}\bar q_\sN^G$ and $\bar u_\sN^G$, respectively. The code takes as input a function that evaluates $\pd{y}\bar{q}$, and scalar values for $\pd{y}\bar\vartheta^\pm$.
\item Construct all the matrices {\bf B}, {\bf M}, {\bf L}, $\bar{\mat{U}}$, $\bar{\mat{Q}}_y$
\item Construct the matrices in (36) and pass the whole thing to a generalized eigenvalue solver, searcing for the eigenvalue with largest imaginary part. If the imaginary part of the eigenvalue is positive then there is exponential growth with rate $k_x$ times the imaginary part of $c$.
\end{itemize}

\noindent{\bf Eady}
The Eady and `Green' problems simplify considerably, which is why they were considered in RYG16.
In both the Eady and Green problems $\bar{q} = 0$, which implies $\bar{\mat{Q}}=0$.
In the Eady problem we also have $\beta=0$.
In the Eady problem the generalized eigenvalue problem reduces to 
\begin{multline}
\left[\begin{array}{c|c|c}
\bar{u}_\sN^G(z=1)+(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^+)&-(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B}))&-(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^-)\\\hline
0&\bar{\mat{U}}&0\\\hline
 (\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^+)&-(\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\left((k_x^2+k_y^2)\mat{M}+\mat{L}\right)^{-1}\mat{B}))&\bar{u}_\sN^G(z=0)-(\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^-)
\end{array}\right]\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{q}\\\hat{\vartheta}^-\end{array}\right) = \\
c\left[\begin{array}{c|c|c}
1&0&0\\\hline
0&\mat{B}&0\\\hline
0&0&1
\end{array}\right]\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{q}\\\hat{\vartheta}^-\end{array}\right)
\end{multline}
The spectrum can be decomposed into two parts.
The first part satisfies $\bar{\mat{U}}\hat{\bm{q}} = c\mat{B}\bar{\bm{q}}$.
Once these eigenvalues and vectors are obtained, you can plug them back in to solve for $\hat{\vartheta}^\pm$.
We expect this part of the problem to be stable, i.e.~all eigenvalues $c$ should be real, or have negative imaginary part.
This can be tested just by computing the eigenvalues of the generalized eigenvalue problem $\bar{\mat{U}}\hat{\bm{q}} = c\mat{B}\bar{\bm{q}}$.

The second part has $\hat{\bm{q}} = 0$, leaving only the $2\times 2$ subsystem
\begin{equation}
\left[\begin{array}{cc}
\bar{u}_\sN^G(z=1)+(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^+) & -(\pd{y}\bar\vartheta^+)(\bm{p}^+\cdot\bm{\psi}^-)\\
 (\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^+)&\bar{u}_\sN^G(z=0)-(\pd{y}\bar\vartheta^-)(\bm{p}^-\cdot\bm{\psi}^-)
\end{array}\right]\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{\vartheta}^-\end{array}\right) = c\left(\begin{array}{c}\hat{\vartheta}^+\\\hat{\vartheta}^-\end{array}\right).
\end{equation}
This is where we expect instability, and eigenvalues are much easier to compute for this $2\times2$ system.

\clearpage
\noindent{\bf Ocean-Charney}
There are lots of other `canonical' problems that we could do, like the Phillips problem which has $\beta\neq0$, $N^2$ constant, and $\bar u\propto \cos(\pi z)$.
We want a problem that will stress our method, i.e.~where the solution is hard to resolve as a function of $z$.
One way to accomplish this is with an ocean-Charney problem where the unstable modes are surface-intensified.
We also want to demonstrate that we can handle non-constant stratification.
One common non-constant stratification is to use an exponential profile for $N^2(z)$.
Our ocean-Charney configuration with exponential stratification is
\[\bar u = \frac{1}{54} \left(3 e^{6 z} (6 z-1)-2 e^6-1\right), \;\;\bar{q} = -2y,\;\;\beta = 1,\;\;\frac{f_0^2}{H^2N^2} = e^{-6z},\;\;\bar\vartheta^+ = -2y,\;\;\bar{\vartheta}^-=0.\]
There are two main questions: how rapidly does the eigenvalue converge as a function of $\sN$ at fixed $k$ (e.g. for the fastest-growing mode), and how does the range of accurate growth rates grow with $\sN$.\\

\hrule
\begin{center}{\bf Linear stability problem: Finite Difference}\end{center}
The standard vertical discretization method for both linear and nonlinear QG is a finite difference approximation.
Details can be found almost anywhere, but there's a treatment in Grooms \& Nadeau (Fluids, 2016) that explicitly considers the treatment of surface buoyancy.
The usual approach uses unequal spacing in the vertical direction with a goal of maximizing the accuracy for a fixed number of vertical levels by putting the resolution preferentially in places where it's needed.
The method has been shown rigorously to converge in the absence of surface buoyancy.
The drawback is that the approximation reduces to first order, so we will only consider an equispaced vertical grid here.
The main reasons why the finite difference method is ubiquitous in QG are that it is easy to implement and that it exactly conserves a discrete energy.

Let $\Delta_z = 1/\sN$ be the grid spacing where $\sN$ is the number of vertical levels.
Both $\psi$ and $q$ are tracked at $\sN$ points starting at $z_1=\Delta_z/2$ and ending at $z_\sN=1-\Delta_z/2$.
The finite difference approximation to $\nabla^2\psi+\pd{z}(S(z)\pd{z}\psi)$ at an interior point $z_k$ ($k\neq1,\sN$) is
\begin{equation}
\left(\nabla^2\psi+\pd{z}(S(z)\pd{z}\psi)\right)|_{z=z_k} \approx\nabla^2\psi_k+ \frac{1}{\Delta_z}\left[S_k\frac{\psi_{k+1}-\psi_k}{\Delta_z}-S_{k-1}\frac{\psi_k-\psi_{k-1}}{\Delta_z}\right] = q_k.
\end{equation}
I've introduced the notation $S_k = S(k\Delta_z)$.
At the boundaries we have the following approximations
\begin{gather}
\left(\nabla^2\psi+\pd{z}(S(z)\pd{z}\psi)\right)|_{z=z_1} \approx \nabla^2\psi_1+\frac{1}{\Delta_z}\left[S_1\frac{\psi_{2}-\psi_1}{\Delta_z}-\vartheta^-\right] = q_1\\
\left(\nabla^2\psi+\pd{z}(S(z)\pd{z}\psi)\right)|_{z=z_\sN} \approx \nabla^2\psi_\sN+\frac{1}{\Delta_z}\left[\vartheta^+-S_{\sN-1}\frac{\psi_\sN-\psi_{\sN-1}}{\Delta_z}\right] = q_\sN.
\end{gather}

As discussed in Grooms \& Nadeau (2016), if we define
\begin{gather}
Q_1 = q_1+\frac{\vartheta^-}{\Delta_z} = \nabla^2\psi_1+\frac{1}{\Delta_z}\left[S_1\frac{\psi_{2}-\psi_1}{\Delta_z}\right],\\
Q_\sN = q_\sN-\frac{\vartheta^+}{\Delta_z} = \nabla^2\psi_\sN-\frac{1}{\Delta_z}\left[S_{\sN-1}\frac{\psi_\sN-\psi_{\sN-1}}{\Delta_z}\right]
\end{gather}
Then the fully nonlinear system dynamics are controlled entirely by the following system
\begin{gather}
\pd{t}Q_1 + \tJ[\psi_\sN,Q_1] + \beta\pd{x}\psi_1 = 0\\
\pd{t}q_k + \tJ[\psi_k,Q_k] + \beta\pd{x}\psi_k = 0,\;\;k = 2,\ldots,\sN-1\\
\pd{t}Q_\sN + \tJ[\psi_\sN,Q_\sN] + \beta\pd{x}\psi_\sN = 0.
\end{gather}
The only caveat is that by evolving this system you can't distinguish $\vartheta^\pm$ or $q_1$, $q_\sN$, but the dynamics of $\psi_k$ are completely controlled by the above system: (42)--(44) for the dynamics and (37), (40), (41) for the PV inversion.\\

The discrete version of the linear stability problem is straightforward in the finite difference approximation.
We can start with (27)--(30) and then discretize as described above.
The discrete version is
\[\left[\bar{\mat{U}}_{FD}\left((k_x^2+k_y^2)\mat{I}+\mat{L}_{FD}\right)-\left(\bar{\mat{Q}}_{y,FD}+ \beta\mat{I}\right)\right]\vec{\psi} = c\left[(k_x^2+k_y^2)\mat{I}+\mat{L}_{FD}\right]\vec{\psi}.\]

The matrix $\bar{\mat{U}}_{FD}$ is diagonal with diagonal elements $\bar u(z_k)$.
The matrix $\mat{L}_{FD}$ is tridiagonal with the form
\[\mat{L}_{FD} = \frac{1}{\Delta_z^2}\left[\begin{array}{ccccc}
S_1&-S_1&0&\cdots&0\\
-S_1&S_1+S_2&-S_2&&\vdots\\
\vdots&\ddots&\ddots&\ddots&\vdots\\
&-S_{k-1}&S_{k-1}+S_k&-S_k&\\
\vdots&\ddots&\ddots&\ddots&\vdots\\
0&\cdots&0&-S_{\sN-1}&S_{\sN-1}
\end{array}\right]\]

The matrix $\bar{\mat{Q}}_{y,FD}$ is also diagonal.
If we define a vector $\bar{\bm{u}}$ whose elements are $\bar u(z_k)$, the diagonal elements of $\bar{\mat{Q}}_{y,FD}$ are the elements of the vector $\mat{L}_{FD}\bar{\bm{u}}$.

\end{document}